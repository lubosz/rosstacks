#!/usr/bin/env python
from __future__ import with_statement

import sys
import yaml
import urllib2
import re

# New-style BSD
# http://www.crummy.com/software/BeautifulSoup/
from BeautifulSoup import BeautifulSoup, Tag , NavigableString

def _load_yaml(filename):
    with open(filename, 'r') as f:
        s = f.read()
    # only support one document
    return [d for d in yaml.load_all(s)][0]
    
def _cleanup_wiki_parsetree(soup):
    # filter out menus, span markup, etc...
    # - span markup
    span_anchors = soup.findAll('span', **{'class': "anchor"})
    [a.extract() for a in span_anchors]
    # - ToC
    res = soup.findAll('div', **{'class': 'table-of-contents'})
    [r.extract() for r in res]
    # - Include menus
    res = soup.findAll('div', id=re.compile('^Include_Menus'))
    [r.extract() for r in res]
    # - Search results
    res = soup.findAll('div', **{'class': 'searchresults'})
    [r.extract() for r in res]

    # remove manifest header
    res = soup.findAll('div', **{'class': 'manifest'})
    [r.extract() for r in res]

    # remove templated links to autogenerated documentation and Troubleshooting
    res = soup.findAll('a', text="auto-generated code documentation")
    [r.extract() for r in res]
    res = soup.findAll('a', text="Troubleshooting")
    [r.extract() for r in res]
    
    
    # clear out package review template stuff
    res = soup.findAll(text="Package review meeting notes")
    for r in res:
        r.parent.parent.extract()
    res = soup.findAll(text="Create new package review")
    for r in res:
        t = r.parent.parent
        sibs = [t.nextSibling, t.nextSibling.nextSibling]
        t.extract()
        for s in sibs:
            s.extract()
    
    # - cleanout empty paragraphs
    res = soup.findAll('p')
    for r in res:
        text = ''.join([str(c) for c in r.contents])
        if not text.strip():
            r.extract()
    
def _pushdown_headers(soup, level):
    #all headers need to be recontexted to current level
    for hl in reversed(xrange(1, 6)):
        res = soup.findAll('h%s'%hl)
        for r in res:
            r.name = 'h%s'%(hl+level)
            r.attrs = []

def _fetch_pages(d, level=1):
    for entry in d:
        assert isinstance(entry, dict)
        for k, v in entry.iteritems():
            if type(v) == list:
                _fetch_pages(v, level+1)
                continue
            if v == None:
              entry[k] = "Empty section"
              continue
            if v.strip().startswith('http:'):
                try:
                    p = urllib2.urlopen(v)
                except urllib2.HTTPError:
                    print "Failed to fetch", v
                    sys.exit(-1)
                # replace contents of d with text of p
                p_text = p.read()
                soup = BeautifulSoup(p_text)

                # remove markup we don't want
                _cleanup_wiki_parsetree(soup)

                # push-down
                _pushdown_headers(soup, level)
                
                # scan to the content div of wiki
                contents = soup.findAll('div', id="content")
                
                content = contents[0]
                p_text = str(content)
                entry[k] = p_text
            else:
                # this is just text, leave it be
                pass
        
def _anchor_key(k):
    return k.replace(' ', '_')

def _sub_convert_to_html(entry, level=1):
    assert len(entry.keys()) == 1
    k = entry.keys()[0]
    safe_k = _anchor_key(k)

    # yield anchor and header
    yield '<a name="%s"></a>'%safe_k
    yield '<h%s>%s</h%s>\n'%(level, k, level)
    
    val = entry[k]
    if type(val) == list:
        # yield each sub entry
        for v in val:
            for l in _sub_convert_to_html(v, level+1):
                yield l
    else:
        # yield the text
        assert isinstance(val, basestring)
        yield val+'\n'
    
def _sub_convert_toc_to_html(doc, depth, max_depth):
    if depth > max_depth:
        return
    for entry in doc:
        assert isinstance(entry, dict)
        for k, v in entry.iteritems():
            yield '<li><a href="#%s">%s</a>'%(_anchor_key(k), k)
            if type(v) == list:
                yield '<ul>'
                for l in _sub_convert_toc_to_html(v, depth+1, max_depth):
                    yield l
                yield '</ul>'
            yield '</li>'
    
def _convert_to_html(doc):
    with open('manual.html', 'w') as f:
        f.write('<html><head><title>ROS Users Manual</title></head>\n')
        f.write('<body>\n')
        f.write('<h1>ROS Users Manual</h1>\n')
        f.write('<h2>Table of Contents</h2>\n')
        for l in _sub_convert_toc_to_html(doc, depth=1, max_depth=3):
            f.write(l+'\n')

        for entry in doc:
            for l in _sub_convert_to_html(entry, level=2):
                f.write(l+'\n')
        f.write('</body></html>')
    
def load_manual(filename):
    d = _load_yaml(filename)
    _fetch_pages(d, level=2)
    _convert_to_html(d)
    #TODO:
    #_convert_to_latex(d)

def rosmanual_main():
    import optparse
    parser = optparse.OptionParser(usage="usage: rosmanual <manual-yaml>")
    options, args = parser.parse_args()
    if len(args) != 1:
        parser.error("you must specify one and only one filename")
    load_manual(args[0])

if __name__ == '__main__':
    rosmanual_main()
